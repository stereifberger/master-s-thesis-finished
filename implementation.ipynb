{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Overview\n",
    "An enviroment to train and evaluate neural networks on learning logical consequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ggogle collab\n",
    "!git clone https://github.com/stereifberger/master-s-thesis\n",
    "%cd master-s-thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies - not necessary on google colab\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Create dataset\n",
    "First the dataset for training is generated. For this the function \"create_dataset\" from \"generation.py\" utilizes the functions \"gen_outp_PA\" to generate a set of random starting formulas, for which iterativly the applicability of rules is checked. All applicable rules are then used to generate new derivations. In each iteration of gen_oupt_PA, set by the iterations variable, new, longer examples are generated.\n",
    "\n",
    "**Rules.** The rules are defined in calculi.py. Two sets are avaiable: Intuitionistic propositional logic (set below via \"calculus = ipl\") and classical propositional logic (set below via \"calculus = cpl\").\n",
    "\n",
    "**Dataset entries.**\n",
    "- **x_train.** Training input: [INDEX, PREMISES, DERIVATION SYMBOL, CONCLUSION]\n",
    "- **y_train_ordered.** Dataset of correct derivations where each sublist i correspnds to INDEX: [DERIVATIONS_0...DERIVATION_N]\n",
    "\n",
    "**Encoding.** Propositional variables and logical constants are encoded as integers. The integers are then one-hot-encoded into unique sequences containing only 0s and ones with the length of the maximum integer value, the feature length. The shape of the individual entries is 2D: [SEQUENCE LENGTH, FEATURE LENGTH].\n",
    "\n",
    "**Example entries withouth numerical representation and one-hot-encoding.**\n",
    "- **x_train.** [2345, A, A THEN B, DERIVES, B OR C]\n",
    "- **y_train_ordered.** Sublist 2345 is entry entry: [[A, A THEN B, B, B OR C], [A, A THEN B, B, A AND B, B OR C]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "x_train_2d, x_train_3d, y_train_ordered, max_y_train_len = generation.create_dataset(iterations = [1,3], calculus = calculi.ipl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Prepare dataset and define model for training\n",
    "Next with pytorch's dataloader the single training entries in x_train are assigned to batches of size \"batch size\" in mixed order. Then the different models are defined using definitions from \"architectures.py\". These models are:\n",
    "\n",
    "- Feedforward network (net)\n",
    "- Recurrent neural network (RNNNet)\n",
    "- Long-short-term memory (LSTMNet)\n",
    "- Transformers (TransformerModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use when gpu is present to empty its catch and define it as \"device\" for referencing it\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the datasets' shapes for th model definitions later\n",
    "two_d_shape = x_train_2d.shape\n",
    "three_d_shape = x_train_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train-test split to 80-20 and get the 2d dataset's shapes # [^1]\n",
    "train_size = int(0.8 * len(x_train_2d)) \n",
    "test_size = len(x_train_2d) - train_size \n",
    "x_train_2d, x_test_2d = random_split(x_train_2d, [train_size, test_size])\n",
    "train_size = int(0.8 * len(x_train_3d))\n",
    "test_size = len(x_train_3d) - train_size\n",
    "x_train_3d, x_test_3d = random_split(x_train_3d, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect and mix the data in [^2]\n",
    "train_dataloader_2d = DataLoader(dataset = x_train_2d, shuffle = True, batch_size = 50)\n",
    "test_dataloader_2d = DataLoader(dataset = x_test_2d, shuffle = True, batch_size = 50)\n",
    "train_dataloader_3d = DataLoader(dataset = x_train_3d, shuffle = True, batch_size = 50)\n",
    "test_dataloader_3d = DataLoader(dataset = x_test_3d, shuffle = True, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the four models [^3]\n",
    "ffn_model = architectures.ffn(input_size = two_d_shape[1]-1, \n",
    "                              output_size = max_y_train_len)\n",
    "rnn_model = architectures.rnn(input_size = three_d_shape[2],\n",
    "                              hidden_size = 20,\n",
    "                              output_size = max_y_train_len)\n",
    "lst_model = architectures.lst(input_size = three_d_shape[2],\n",
    "                              hidden_size = 20,\n",
    "                              output_size = max_y_train_len)\n",
    "tra_model = architectures.tra(input_size = three_d_shape[2],\n",
    "                              hidden_size = 20,\n",
    "                              output_size = max_y_train_len,\n",
    "                              num_layers = 2,\n",
    "                              nhead = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer to be SGD, with a learning rate of 0.1 [^4]\n",
    "ffn_optimizer = torch.optim.SGD(ffn_model.parameters(),lr=0.001)\n",
    "rnn_optimizer = torch.optim.SGD(rnn_model.parameters(),lr=0.001)\n",
    "lst_optimizer = torch.optim.SGD(lst_model.parameters(),lr=0.001)\n",
    "tra_optimizer = torch.optim.SGD(tra_model.parameters(),lr=0.001)\n",
    "\n",
    "# Train for 1000 epochs\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Training\n",
    "Each subsequent cell trains one of the four models and calculates their mean squared error loss for the nearest correct derivation from the dataset to the derivation provided by the model. The logic for this is impolemented in the custom loss function \"mse_min_dist\" in losses.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feedforward model to the gpu \n",
    "ffn_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ground truth data to the gpu\n",
    "y_train = y_train_ordered.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_y_length = int(max_y_train_len/14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FFN ### [^5]\n",
    "ffn_costval_train = [] # Define the lists for the loss values\n",
    "ffn_costval_test = []\n",
    "for j in tqdm(range(epochs), desc = \"Epoch\"): # Loop over all epochs\n",
    "    ffn_model.train() # Set to training mode (weights are adjusted)\n",
    "    train_loss = 0\n",
    "    for i, x_train in enumerate(train_dataloader_2d):   # Loop over all batches\n",
    "        x_train = x_train.to(device)\n",
    "        y_pred = ffn_model(x_train[:,1:])               # Get the model's output for batch\n",
    "        cost = losses.nffn_mse_min_dist(y_pred, x_train, y_train, max_y_length) # Calculate loss\n",
    "        # Backpropagation\n",
    "        ffn_optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        ffn_optimizer.step()\n",
    "        train_loss += cost.item() # Append loss to intermediary list for average loss calculation\n",
    "    avg_train_loss = train_loss / len(train_dataloader_2d) # Calculate average loss\n",
    "    ffn_costval_train.append(avg_train_loss)\n",
    "\n",
    "    ffn_model.eval() # Set evaluation mode (weights are not adjusted)\n",
    "    test_loss = 0\n",
    "    # Analog to above but without training a loop over all batches\n",
    "    with torch.no_grad():\n",
    "        for i, x_test in enumerate(test_dataloader_2d):\n",
    "            x_test = x_test.to(device)\n",
    "            y_pred = ffn_model(x_test[:, 1:])\n",
    "            cost = losses.nffn_mse_min_dist(y_pred, x_test, y_train, max_y_length)\n",
    "            test_loss += cost.item()\n",
    "    avg_test_loss = test_loss / len(test_dataloader_2d)\n",
    "    ffn_costval_test.append(avg_train_loss)\n",
    "\n",
    "    if j % 10 == 0: # Get the loss every 10 epochs\n",
    "        print(f\"Epoch {j}: Train Loss - {avg_train_loss}, Test Loss - {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RNN ###\n",
    "rnn_costval_train = [] # Define the lists for the loss values\n",
    "rnn_costval_test = []\n",
    "for j in tqdm(range(epochs), desc = \"Epoch\"): # Loop over all epochs\n",
    "    rnn_model.train() # Set to training mode (weights are adjusted)\n",
    "    train_loss = 0\n",
    "    for i, x_train in enumerate(train_dataloader_3d):   # Loop over all batches\n",
    "        x_train = x_train.to(device)\n",
    "        y_pred = rnn_model(x_train[:,1:])               # Get the model's output for batch \n",
    "        cost = losses.threed_mse_min_dist(y_pred, x_train, y_tdict, (max_y_train_len/14), \"rnn\") # Calculate loss\n",
    "        # Backpropagation\n",
    "        rnn_optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        rnn_optimizer.step()\n",
    "        train_loss += cost.item() # Append loss to intermediary list for average loss calculation\n",
    "    avg_train_loss = train_loss / len(train_dataloader_3d) # Calculate average loss\n",
    "    rnn_costval_train.append(avg_train_loss)\n",
    "\n",
    "    rnn_model.eval() # Set evaluation mode (weights are not adjusted)\n",
    "    test_loss = 0 \n",
    "    # Analog to above but without training a loop over all batches\n",
    "    with torch.no_grad(): \n",
    "        for i, x_test in enumerate(test_dataloader_3d):\n",
    "            x_test = x_test.to(device)\n",
    "            y_pred = rnn_model(x_test[:, 1:])\n",
    "            cost = losses.threed_mse_min_dist(y_pred, x_test, y_tdict, (max_y_train_len/14), \"rnn\")\n",
    "            test_loss += cost.item()\n",
    "    avg_test_loss = test_loss / len(test_dataloader_2d)\n",
    "    rnn_costval_test.append(avg_train_loss)\n",
    "\n",
    "    if j % 10 == 0: # Get the loss every 10 epochs\n",
    "        print(f\"Epoch {j}: Train Loss - {avg_train_loss}, Test Loss - {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSTM ###\n",
    "lst_costval_train = [] # Define the lists for the loss values\n",
    "lst_costval_test = []\n",
    "for j in tqdm(range(epochs), desc = \"Epoch\"): # Loop over all epochs\n",
    "    lst_model.train() # Set to training mode (weights are adjusted)\n",
    "    train_loss = 0\n",
    "    for i, x_train in enumerate(train_dataloader_3d):   # Loop over all batches\n",
    "        x_train = x_train.to(device)\n",
    "        y_pred = lst_model(x_train[:,1:])               # Get the model's output for batch \n",
    "        cost = losses.threed_mse_min_dist(y_pred, x_train, y_tdict, (max_y_train_len/14), \"lst\") # Calculate loss\n",
    "        # Backpropagation\n",
    "        lst_optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        lst_optimizer.step()\n",
    "        train_loss += cost.item() # Append loss to intermediary list for average loss calculation\n",
    "    avg_train_loss = train_loss / len(train_dataloader_3d) # Calculate average loss\n",
    "    lst_costval_train.append(avg_train_loss)\n",
    "\n",
    "    lst_model.eval() # Set evaluation mode (weights are not adjusted)\n",
    "    test_loss = 0 \n",
    "    # Analog to above but without training a loop over all batches\n",
    "    with torch.no_grad(): \n",
    "        for i, x_test in enumerate(test_dataloader_3d):\n",
    "            x_test = x_test.to(device)\n",
    "            y_pred = lst_model(x_test[:, 1:])\n",
    "            cost = losses.threed_mse_min_dist(y_pred, x_test, y_tdict, (max_y_train_len/14), \"lst\")\n",
    "            test_loss += cost.item()\n",
    "    avg_test_loss = test_loss / len(test_dataloader_2d)\n",
    "    lst_costval_test.append(avg_train_loss)\n",
    "\n",
    "    if j % 10 == 0: # Get the loss every 10 epochs\n",
    "        print(f\"Epoch {j}: Train Loss - {avg_train_loss}, Test Loss - {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformer ###\n",
    "tra_costval_train = [] # Define the lists for the loss values\n",
    "tra_costval_test = []\n",
    "for j in tqdm(range(epochs), desc = \"Epoch\"): # Loop over all epochs\n",
    "    tra_model.train() # Set to training mode (weights are adjusted)\n",
    "    train_loss = 0\n",
    "    for i, x_train in enumerate(train_dataloader_3d):   # Loop over all batches\n",
    "        x_train = x_train.to(device)\n",
    "        y_pred = tra_model(x_train[:,1:])               # Get the model's output for batch \n",
    "        cost = losses.mse_min_dist(y_pred, x_train, y_tdict, (max_y_train_len/14), \"tra\") # Calculate loss\n",
    "        # Backpropagation\n",
    "        tra_optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        tra_optimizer.step()\n",
    "        train_loss += cost.item() # Append loss to intermediary list for average loss calculation\n",
    "    avg_train_loss = train_loss / len(train_dataloader_3d) # Calculate average loss\n",
    "    tra_costval_train.append(avg_train_loss)\n",
    "\n",
    "    tra_model.eval() # Set evaluation mode (weights are not adjusted)\n",
    "    test_loss = 0 \n",
    "    # Analog to above but without training a loop over all batches\n",
    "    with torch.no_grad(): \n",
    "        for i, x_test in enumerate(test_dataloader_3d):\n",
    "            x_test = x_test.to(device)\n",
    "            y_pred = tra_model(x_test[:, 1:])\n",
    "            cost = losses.mse_min_dist(y_pred, x_test, y_tdict, (max_y_train_len/14), \"tra\")\n",
    "            test_loss += cost.item()\n",
    "    avg_test_loss = test_loss / len(test_dataloader_2d)\n",
    "    tra_costval_train.append(avg_train_loss)\n",
    "\n",
    "    if j % 10 == 0: # Get the loss every 10 epochs\n",
    "        print(f\"Epoch {j}: Train Loss - {avg_train_loss}, Test Loss - {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Plot results\n",
    "Here all results from above are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "x_data = list(range(200))\n",
    "y_data_ffn = ffn_costval_train\n",
    "y_data_rnn = rnn_costval_train\n",
    "y_data_lst = lstm_costval_train\n",
    "y_data_tra = tra_costval_train\n",
    "plt.plot(x_data, y_data_ffn, label='FFN')\n",
    "plt.plot(x_data, y_data_rnn, label='RNN')\n",
    "plt.plot(x_data, y_data_lst, label='LSTM')\n",
    "plt.plot(x_data, y_data_tra, label='Transformers')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "x_data = list(range(200))\n",
    "y_data_ffn = ffn_costval_train\n",
    "y_data_rnn = rnn_costval_train\n",
    "y_data_lst = lstm_costval_train\n",
    "y_data_tra = tra_costval_train\n",
    "plt.plot(x_data, y_data_ffn, label='FFN')\n",
    "plt.plot(x_data, y_data_rnn, label='RNN')\n",
    "plt.plot(x_data, y_data_lst, label='LSTM')\n",
    "plt.plot(x_data, y_data_tra, label='Transformers')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
