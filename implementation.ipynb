{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Overview\n",
    "This is an enviroment to trai and evaluate neural networks on learning logical calculi in a propositional logic. First the needed files are cloned from GitHub for it to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Collab: Get repository and go to it in collab.\n",
    "!git clone https://github.com/stereifberger/master-s-thesis\n",
    "%cd master-s-thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab if above does not move to right directory\n",
    "%cd /content/master-s-thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Jupyter Notebook (also in VSCode) after starting Jupyter server: Go to right directory.\n",
    "%cd master-s-thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies - not necessary on Google Colab\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries from imports.py\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Create dataset\n",
    "First the dataset for training is generated. For this the function \"create_dataset\" from \"generation.py\" utilizes the functions \"gen_outp_PA\" to generate a set of random starting formulas, for which iterativly the applicability of rules is checked. All applicable rules are then used to generate new derivations. In each iteration of gen_oupt_PA, set by the iterations variable, new, longer examples are generated. get_conclusions generates some random conclusions using approximately the same procedure. The set of derivations is then filtered down to ones only containing those.\n",
    "\n",
    "**Rules.** The rules are defined in calculi.py. Two sets are avaiable: Intuitionistic propositional logic (set below via \"calculus = ipl\") and classical propositional logic (set below via \"calculus = cl\").\n",
    "\n",
    "**Dataset entries.**\n",
    "- **x_trai.** traiing input: [INDEX, PREMISES, CONCLUSION]\n",
    "- **y_trai_ordered.** Dataset of correct derivations where each sublist i correspnds to INDEX: [DERIVATIONS_0...DERIVATION_N]\n",
    "\n",
    "**Encoding.** Propositional variables and logical constants are encoded as integers. The integers are then one-hot-encoded into unique sequences containing only 0s and ones with the length of the maximum integer value, the feature length. The shape of the individual entries is 2D: [SEQUENCE LENGTH, FEATURE LENGTH].\n",
    "\n",
    "**Example entries withouth numerical representation and one-hot-encoding.**\n",
    "- **x_trai.** [2345, A, A THEN B, B OR C]\n",
    "- **y_trai_ordered.** Sublist 2345 is entry entry: [[A, A THEN B, B, B OR C], [A, A THEN B, B, A AND B, B OR C]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input (as two and three dimensional vector) and output datset as outputs of generation.create_datset\n",
    "# It applies rules of classical logic up to two times to do so on randomly generated premises\n",
    "x_trai_2d, x_trai_3d, y_trai_ordered, max_y_trai_len = generation.create_dataset(iterations = [1,2], calculus = calculi.cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x_trai_2d, 'x_trai_2d.pt')                 # Save two dimensional input dataset to backup file\n",
    "torch.save(x_trai_3d, 'x_trai_3d.pt')                 # Save three dimensional input dataset to backup file\n",
    "torch.save(y_trai_ordered, 'y_trai_ordered.pt')       # Save output dataset to backup file\n",
    "with open('Medium_max_y_trai_len.json', 'w') as file: # Save number with maximum length of derivations in output dataset to backup file\n",
    "    json.dump(max_y_trai_len, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Prepare dataset and define models for traiing\n",
    "Next with pytorch's dataloader the single traiing entries in x_trai are assigned to batches of size \"batch size\" in mixed order. Then the different models are defined using definitions from \"architectures.py\". These models are:\n",
    "\n",
    "- Feedforward network (net)\n",
    "- Recurrent neural network (RNNNet)\n",
    "- Long-short-term memory (LSTMNet)\n",
    "- Transformers (TransformerModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():                                           # Empty avaiable GPU memory when GPU is present\n",
    "    torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   # GPU is defined as 'device' when present for using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_d_shape = x_trai_2d.shape               # Get the 2d traiing dataset's shape for the model definitions later\n",
    "thr_d_shape = x_trai_3d.shape               # Get the 23 traiing dataset's shape for the model definitions later\n",
    "max_y_length = int(max_y_trai_len/14)       # Define maximum length of derivations for non one-hot encoded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.argmax(x_trai_3d, dim=2)          # Reverse one-hot encoding for encoder-decoder models\n",
    "x[:, 0] = x_trai_2d[:, 0]\n",
    "x_trai_nu = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trai_si = int(0.8 * len(x_trai_2d))                                     # Set trai-test split to 80-20 [^1]\n",
    "test_si = len(x_trai_2d) - trai_si\n",
    "x_trai_2d, x_test_2d = random_split(x_trai_2d, [trai_si, test_si])      # Choose random derivations from datsets distributed by trai-test split\n",
    "x_trai_3d, x_test_3d = random_split(x_trai_3d, [trai_si, test_si])\n",
    "x_trai_nu, x_test_nu = random_split(x_trai_nu, [trai_si, test_si])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trai_dl_2d = DataLoader(dataset = x_trai_2d, shuffle = True, batch_size = 16)   # Create batches of size 16 from datasets\n",
    "test_dl_2d = DataLoader(dataset = x_test_2d, shuffle = True, batch_size = 16)\n",
    "trai_dl_3d = DataLoader(dataset = x_trai_3d, shuffle = True, batch_size = 16)\n",
    "test_dl_3d = DataLoader(dataset = x_test_3d, shuffle = True, batch_size = 16)\n",
    "trai_dl_nu = DataLoader(dataset = x_trai_nu, shuffle = True, batch_size = 64)\n",
    "test_dl_nu = DataLoader(dataset = x_test_nu, shuffle = True, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trai = y_trai_ordered.to(device)                      # Load ground truth data to GPU\n",
    "y_trai_3d = y_trai.view(int(len(y_trai)),               # Reshape training 3d data\n",
    "                        int(len(y_trai[0])),\n",
    "                        int(len(y_trai[0][0])/14), 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experiments the below parameters of the architectures have to be specified in accordance to which one is test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder-Decoder networks\n",
    "## FFN | Inputs: input_dim, hidden dim\n",
    "encoder_ffn = architectures.Encoder_FFN(thr_d_shape[1], 150)\n",
    "decoder_ffn = architectures.Decoder_FFN((max_y_length*14), 150)\n",
    "ffn_ed_model = architectures.Seq2Seq(encoder_ffn, decoder_ffn, device)\n",
    "## LSTM | Inputs: input_dim, embedding dim, hidden dim, nr layers, droput\n",
    "encoder_lstm = architectures.Encoder_LSTM(thr_d_shape[1], 150, 150, 1, 0)\n",
    "decoder_lstm = architectures.Decoder_LSTM(14, 150, 150, 3, 0)\n",
    "lst_ed_model = architectures.Seq2Seq(encoder_lstm, decoder_lstm, device)\n",
    "## Transformer-Encoder | Inputs:  input_dim, emb_dim, num_heads, hidden_dim, num_layers, dropout\n",
    "encoder_tra = architectures.TransformerEncoder(thr_d_shape[1], 150, 5, 150, 1, dropout=0)\n",
    "# Transformer-Decoder | Inputs: output_dim, emb_dim, num_heads, hidden_dim, num_layers\n",
    "decoder_tra = architectures.TransformerDecoder(14, 150, 5, 150, 3)\n",
    "tra_ed_model = architectures.Seq2SeqTransformer(encoder_tra, decoder_tra, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001                                                              # Define Learning rate\n",
    "ffn_ed_optimizer = torch.optim.AdamW(ffn_ed_model.parameters(),lr=lr)   # Define optimizers based on AdamW for models\n",
    "lst_ed_optimizer = torch.optim.AdamW(lst_ed_model.parameters(),lr=lr)\n",
    "tra_ed_optimizer = torch.optim.AdamW(tra_ed_model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 FFN Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn_ed_model.to(device)                                                 # Load model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tain model and save results\n",
    "FFN_CELtrai, FFN_CELtest, FFN_ACCtrai, FFN_ACCtest = schedule.train_model(model = ffn_ed_model,\n",
    "                                                                         dataloader_train = trai_dl_nu,\n",
    "                                                                         dataloader_test = test_dl_nu,\n",
    "                                                                         optimizer = ffn_ed_optimizer,\n",
    "                                                                         criterion = criterion,\n",
    "                                                                         epochs = 50,\n",
    "                                                                         device = device,\n",
    "                                                                         max_y_length = max_y_length,\n",
    "                                                                         y_train = y_trai)\n",
    "torch.save(ffn_ed_model.state_dict(), 'addition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, output = schedule.sanity(ffn_ed_model, test_dl_nu, device, max_y_length) # A sanity test for wheter the outputs look appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ffn_ed_model            # Delete model from GPU to make space for new models\n",
    "torch.cuda.empty_cache()    # Empty newly avaiable GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 LSTM Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_ed_model.to(device)     # Load model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tain model and save results\n",
    "LSTM_CELtrai, LSTM_CELtest, LSTM_ACCtrai, LSTM_ACCtest = schedule.train_model(model = lst_ed_model,\n",
    "                                                                             dataloader_train = trai_dl_nu,\n",
    "                                                                             dataloader_test = test_dl_nu,\n",
    "                                                                             optimizer = lst_ed_optimizer,\n",
    "                                                                             criterion = criterion,\n",
    "                                                                             epochs = 50,\n",
    "                                                                             device = device,\n",
    "                                                                             max_y_length = max_y_length,\n",
    "                                                                             y_train = y_trai_3d)\n",
    "torch.save(lst_ed_model.state_dict(), 'addition_model.pth')     # Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, output = schedule.sanity(lst_ed_model, test_dl_nu, device, max_y_length) # A sanity test for wheter the outputs look appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lst_ed_model            # Delete model from GPU to make space for new models\n",
    "torch.cuda.empty_cache()    # Empt newly avaiable GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_ed_model.to(device)     # Load model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRA_CELtrai, TRA_CELtest, TRA_ACCtrai, TRA_ACCtest = schedule.train_model(model = tra_ed_model,\n",
    "                                                                         dataloader_train = trai_dl_nu,\n",
    "                                                                         dataloader_test = test_dl_nu,\n",
    "                                                                         optimizer = tra_ed_optimizer,\n",
    "                                                                         criterion = criterion,\n",
    "                                                                         epochs = 50,\n",
    "                                                                         device = device,\n",
    "                                                                         max_y_length = max_y_length,\n",
    "                                                                         y_train = y_trai_3d)\n",
    "torch.save(tra_ed_model.state_dict(), 'addition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.sanity(tra_ed_model, test_dl_nu, device, max_y_length) # A sanity test for wheter the outputs look appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tra_ed_model            # Delete model from GPU to make space for new models\n",
    "torch.cuda.empty_cache()    # Empt newly avaiable GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Plot results\n",
    "Here results from the preset parameters above can be plotted. Results in the thesis are plotted from csv files using tikz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Network,  Medium Dataset, Cross Entropy Loss\n",
    "plt.figure(figsize=(5, 5), dpi=200)\n",
    "x_data = list(range(50))\n",
    "prop = fm.FontProperties(fname='/usr/share/fonts/opentype/freefont/FreeSerif.otf')\n",
    "plt.plot(x_data, FFN_CELtrai, label='Training cross entropy loss')\n",
    "plt.plot(x_data, FFN_CELtest, label='Test cross entropy loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross entropy loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Network, Medium Dataset, Accuracy\n",
    "plt.figure(figsize=(5, 5), dpi=200)\n",
    "x_data = list(range(50))\n",
    "plt.plot(x_data, FFN_ACCtrai, label='Training accuracy')\n",
    "plt.plot(x_data, FFN_ACCtest, label='Test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Short-Term Memory, Medium, Accuracy, Cross Entropy Loss\n",
    "plt.figure(figsize=(5, 5), dpi=200)\n",
    "x_data = list(range(50))\n",
    "plt.plot(x_data, LSTM_CELtrai, label='Training cross entropy loss')\n",
    "plt.plot(x_data, LSTM_CELtest, label='Test cross entropy loss')\n",
    "plt.plot(x_data, LSTM_ACCtrai, label='Traiing accuracy')\n",
    "plt.plot(x_data, LSTM_ACCtest, label='Test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Transformers results')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer, Medium, Accuracy, Cross Entropy Loss\n",
    "plt.figure(figsize=(5, 5), dpi=200)\n",
    "x_data = list(range(50))\n",
    "plt.plot(x_data, TRA_CELtrai, label='Training cross entropy loss')\n",
    "plt.plot(x_data, TRA_CELtest, label='Test cross entropy loss')\n",
    "plt.plot(x_data, TRA_ACCtrai, label='Training accuracy')\n",
    "plt.plot(x_data, TRA_ACCtest, label='Test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Transformers results')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-Accuracy all Networks, Accuracy\n",
    "plt.figure(figsize=(5, 5), dpi=200)\n",
    "x_data = list(range(50))\n",
    "#plt.plot(x_data, FFN_ACCtest, label='FFN Test accuracy')\n",
    "plt.plot(x_data, RNN_ACCtest, label='RNN test accuracy')\n",
    "plt.plot(x_data, RNN_ACCtest, label='RNN test accuracy')\n",
    "plt.plot(x_data, LSTM_ACCtest, label='LSTM test accuracy')\n",
    "plt.plot(x_data, TRA_ACCtest, label='Transformer test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Transformers results')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-Accuracy all Networks, Accuracy\n",
    "plt.figure(figsize=(5, 5), dpi=200)\n",
    "x_data = list(range(50))\n",
    "#plt.plot(x_data, FFN_ACCtest, label='FFN Test accuracy')\n",
    "plt.plot(x_data, RNN_CELtest, label='RNN test loss')\n",
    "plt.plot(x_data, LSTM_CELtest, label='LSTM test loss')\n",
    "plt.plot(x_data, TRA_CELtest, label='Transformer test loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Transformers results')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
