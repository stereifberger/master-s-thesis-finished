{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Overview\n",
    "An enviroment to train and evaluate neural networks on learning logical consequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Collab: Get repository and go to it in collab.\n",
    "!git clone https://github.com/stereifberger/master-s-thesis\n",
    "%cd master-s-thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab if above does not move to right directory\n",
    "%cd /content/master-s-thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For VsCode after starting Jupyter server: go to right directory.\n",
    "%cd master-s-thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies - not necessary on google colab\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reloading libraries.\n",
    "importlib.reload(architectures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Create dataset\n",
    "First the dataset for training is generated. For this the function \"create_dataset\" from \"generation.py\" utilizes the functions \"gen_outp_PA\" to generate a set of random starting formulas, for which iterativly the applicability of rules is checked. All applicable rules are then used to generate new derivations. In each iteration of gen_oupt_PA, set by the iterations variable, new, longer examples are generated.\n",
    "\n",
    "**Rules.** The rules are defined in calculi.py. Two sets are avaiable: Intuitionistic propositional logic (set below via \"calculus = ipl\") and classical propositional logic (set below via \"calculus = cpl\").\n",
    "\n",
    "**Dataset entries.**\n",
    "- **x_train.** Training input: [INDEX, PREMISES, DERIVATION SYMBOL, CONCLUSION]\n",
    "- **y_train_ordered.** Dataset of correct derivations where each sublist i correspnds to INDEX: [DERIVATIONS_0...DERIVATION_N]\n",
    "\n",
    "**Encoding.** Propositional variables and logical constants are encoded as integers. The integers are then one-hot-encoded into unique sequences containing only 0s and ones with the length of the maximum integer value, the feature length. The shape of the individual entries is 2D: [SEQUENCE LENGTH, FEATURE LENGTH].\n",
    "\n",
    "**Example entries withouth numerical representation and one-hot-encoding.**\n",
    "- **x_train.** [2345, A, A THEN B, DERIVES, B OR C]\n",
    "- **y_train_ordered.** Sublist 2345 is entry entry: [[A, A THEN B, B, B OR C], [A, A THEN B, B, A AND B, B OR C]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "x_train_2d, x_train_3d, y_train_ordered, max_y_train_len = generation.create_dataset(iterations = [1,2], calculus = calculi.ipl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "torch.save(x_train_2d, 'x_train_2d.pt')\n",
    "torch.save(x_train_3d, 'x_train_3d.pt')\n",
    "torch.save(y_train_ordered, 'y_train_ordered.pt')\n",
    "with open('Medium_max_y_train_len.json', 'w') as file:\n",
    "    json.dump(max_y_train_len, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Prepare dataset and define models for training\n",
    "Next with pytorch's dataloader the single training entries in x_train are assigned to batches of size \"batch size\" in mixed order. Then the different models are defined using definitions from \"architectures.py\". These models are:\n",
    "\n",
    "- Feedforward network (net)\n",
    "- Recurrent neural network (RNNNet)\n",
    "- Long-short-term memory (LSTMNet)\n",
    "- Transformers (TransformerModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use when gpu is present to empty its catch and define it as \"device\" for referencing it\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the datasets' shapes for the model definitions later\n",
    "two_d_shape = x_train_2d.shape\n",
    "three_d_shape = x_train_3d.shape\n",
    "max_y_length = int(max_y_train_len/14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse one-hot encoding for encoder-decoder models\n",
    "x = torch.argmax(x_train_3d, dim=2) \n",
    "x[:, 0] = x_train_2d[:, 0]\n",
    "x_train_nu = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train-test split to 80-20 [^1]\n",
    "train_size = int(0.8 * len(x_train_2d)) \n",
    "test_size = len(x_train_2d) - train_size \n",
    "x_train_2d, x_test_2d = random_split(x_train_2d, [train_size, test_size])\n",
    "x_train_3d, x_test_3d = random_split(x_train_3d, [train_size, test_size])\n",
    "x_train_nu, x_test_nu = random_split(x_train_nu, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect and mix the data in [^2]\n",
    "train_dataloader_2d = DataLoader(dataset = x_train_2d, shuffle = True, batch_size = 50)\n",
    "test_dataloader_2d = DataLoader(dataset = x_test_2d, shuffle = True, batch_size = 50)\n",
    "train_dataloader_3d = DataLoader(dataset = x_train_3d, shuffle = True, batch_size = 50)\n",
    "test_dataloader_3d = DataLoader(dataset = x_test_3d, shuffle = True, batch_size = 50)\n",
    "train_dataloader_nu = DataLoader(dataset = x_test_nu, shuffle = True, batch_size = 50)\n",
    "test_dataloader_nu = DataLoader(dataset = x_test_nu, shuffle = True, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth data to GPU\n",
    "y_train = y_train_ordered.to(device)\n",
    "y_train_3d = y_train.view(int(len(y_train)), int(len(y_train[0])), int(len(y_train[0][0])/14), 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simple one-hot to one-hot networks [^3]\n",
    "## FFN (onehot to onehot)\n",
    "#ffn_oh_model = architectures.ffn(input_size = two_d_shape[1]-1, \n",
    "#                              hidden_size = 10,\n",
    "#                              output_size = max_y_train_len,\n",
    "#                              dropout_rate = 0.1,\n",
    "#                              input_size_in = 756)\n",
    "## RNN (onehot to onehot)\n",
    "#rnn_oh_model = architectures.SimpleRNN(input_size = three_d_shape[2],\n",
    "#                              hidden_size = 150,\n",
    "#                              output_size = three_d_shape[2])\n",
    "## LSTM (onehot to onehot)\n",
    "#lst_oh_model = architectures.lst(input_size = three_d_shape[2],\n",
    "#                              hidden_size = 150,\n",
    "#                             output_size = max_y_train_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder-Decoder networks\n",
    "## FFN | Inputs: input_dim, hidden dim\n",
    "encoder_ffn = architectures.Encoder_FFN(three_d_shape[1], 150)\n",
    "decoder_ffn = architectures.Decoder_FFN((max_y_length*14), 150)\n",
    "ffn_ed_model = architectures.Seq2Seq(encoder_ffn, decoder_ffn, device)\n",
    "## RNN | Inputs: input_dim, embedding dim, hidden dim, nr layers\n",
    "encoder_rnn = architectures.Encoder_RNN(three_d_shape[1], 150, 150, 1)\n",
    "decoder_rnn = architectures.Decoder_RNN(14, 150, 150, 3)\n",
    "rnn_ed_model = architectures.Seq2Seq(encoder_rnn, decoder_rnn, device)\n",
    "## LSTM | Inputs: input_dim, embedding dim, hidden dim, nr layers, droput\n",
    "encoder_lstm = architectures.Encoder_LSTM(three_d_shape[1], 150, 150, 2, 0.5)\n",
    "decoder_lstm = architectures.Decoder_LSTM(14, 150, 150, 3, 0.5)\n",
    "lst_ed_model = architectures.Seq2Seq(encoder_lstm, decoder_lstm, device)\n",
    "## Transformer | Inputs: input_dim, embedding dim, hidden dim, nr layers, droput\n",
    "encoder_tra = architectures.TransformerEncoder(three_d_shape[1], 150, 5, 150, 1)\n",
    "decoder_tra = architectures.TransformerDecoder(14, 150, 1, 150, 3)\n",
    "tra_ed_model = architectures.Seq2SeqTransformer(encoder_tra, decoder_tra, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers for models\n",
    "lr = 0.001\n",
    "ffn_oh_optimizer = torch.optim.Adam(ffn_oh_model.parameters(),lr=lr)\n",
    "rnn_oh_optimizer = torch.optim.Adam(rnn_oh_model.parameters(),lr=lr)\n",
    "lst_oh_optimizer = torch.optim.Adam(lst_oh_model.parameters(),lr=lr)\n",
    "ffn_ed_optimizer = torch.optim.Adam(ffn_ed_model.parameters(),lr=lr)\n",
    "rnn_ed_optimizer = torch.optim.Adam(rnn_ed_model.parameters(),lr=lr)\n",
    "lst_ed_optimizer = torch.optim.Adam(lst_ed_model.parameters(),lr=lr)\n",
    "tra_ed_optimizer = torch.optim.Adam(tra_ed_model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Encoder-Decoder Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 FFN Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model to GPU\n",
    "ffn_ed_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model and save results\n",
    "FFN_CELtrain, FFN_CELtest, FFN_ACCtrain, FFN_ACCtest = schedule.train_model(ffn_ed_model, train_dataloader_nu, test_dataloader_nu, ffn_ed_optimizer, criterion, 100, device, max_y_length, y_train)\n",
    "torch.save(ffn_ed_model.state_dict(), 'addition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sanity test for wheter the outputs look appropriate\n",
    "schedule.sanity(ffn_ed_model, test_dataloader_nu, device, max_y_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model from GPU to make space for new models\n",
    "del ffn_ed_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 RNN Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model to GPU\n",
    "rnn_ed_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "RNN_CELtrain, RNN_CELtest, RNN_ACCtrain, RNN_ACCtest = schedule.train_model(rnn_ed_model, train_dataloader_nu, test_dataloader_nu, rnn_ed_optimizer, criterion, 100, device, max_y_length, y_train_3d)\n",
    "torch.save(rnn_ed_model.state_dict(), 'addition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sanity test for wheter the outputs look appropriate\n",
    "schedule.sanity(rnn_ed_model, test_dataloader_nu, device, max_y_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model from GPU to make space for new models\n",
    "del rnn_ed_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 LSTM Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model to GPU\n",
    "lst_ed_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "LSTM_CELtrain, LSTM_CELtest, LSTM_ACCtrain, LSTM_ACCtest = schedule.train_model(lst_ed_model, train_dataloader_nu, test_dataloader_nu, lst_ed_optimizer, criterion, 100, device, max_y_length, y_train_3d)\n",
    "torch.save(lst_ed_model.state_dict(), 'addition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sanity test for wheter the outputs look appropriate\n",
    "schedule.sanity(lst_ed_model, test_dataloader_nu, device, max_y_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model from GPU to make space for new models\n",
    "del lst_ed_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model to GPU\n",
    "tra_ed_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRA_CELtrain, TRA_CELtest, TRA_ACCtrain, TRA_ACCtest = schedule.train_model(tra_ed_model, train_dataloader_nu, test_dataloader_nu, tra_ed_optimizer, criterion, 100, device, max_y_length, y_train_3d)\n",
    "torch.save(tra_ed_model.state_dict(), 'addition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sanity test for wheter the outputs look appropriate\n",
    "schedule.sanity(tra_ed_model, test_dataloader_nu, device, max_y_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model from GPU to make space for new models\n",
    "del tra_ed_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Onehot-to-Onehot | NOT WORKING\n",
    "NOT WORKING RIGHT NOW. No encoder-decoder structure but only simple networks that get fed the onehot encoded data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feedforward model to the gpu \n",
    "ffn_oh_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.train_model(ffn_oh_model, train_dataloader_2d, test_dataloader_2d, ffn_oh_optimizer, criterion, 200, device, max_y_length, y_train)\n",
    "torch.save(ffn_oh_model.state_dict(), 'addition_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sanity test for wheter the outputs look appropriate.\n",
    "schedule.sanity_r(ffn_oh_model, test_dataloader_2d, device, max_y_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model from GPU to make space for new models\n",
    "del ffn_oh_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feedforward model to the gpu \n",
    "rnn_oh_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule.train_model(rnn_oh_model, train_dataloader_3d, test_dataloader_3d, ffn_oh_optimizer, criterion, 200, device, max_y_length, y_train_3d)\n",
    "torch.save(rnn_oh_model.state_dict(), 'addition_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FFN ### [^5]\n",
    "ffn_costval_train = [] # Define the lists for the loss values\n",
    "ffn_costval_test = []\n",
    "for j in tqdm(range(50), desc = \"Epoch\"): # Loop over all epochs\n",
    "    ffn_oh_model.train() # Set to training mode (weights are adjusted)\n",
    "    train_loss = 0\n",
    "    for i, x_train in enumerate(train_dataloader_2d):   # Loop over all batches\n",
    "        x_train = x_train.to(device)\n",
    "        y_pred = ffn_oh_model(x_train[:,1:], max_y_length)               # Get the model's output for batch\n",
    "        cost, y_train_collected = losses.nffn_mse_min_dist(y_pred, x_train, y_train, max_y_length, device) # Calculate loss\n",
    "        # Backpropagation\n",
    "        ffn_oh_optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        ffn_oh_optimizer.step()\n",
    "        train_loss += cost.item() # Append loss to intermediary list for average loss calculation\n",
    "    avg_train_loss = train_loss / len(train_dataloader_2d) # Calculate average loss\n",
    "    ffn_costval_train.append(avg_train_loss)\n",
    "\n",
    "    ffn_oh_model.eval() # Set evaluation mode (weights are not adjusted)\n",
    "    test_loss = 0\n",
    "    # Analog to above but without training a loop over all batches\n",
    "    with torch.no_grad():\n",
    "        for i, x_test in enumerate(test_dataloader_2d):\n",
    "            x_test = x_test.to(device)\n",
    "            y_pred = ffn_oh_model(x_test[:, 1:], max_y_length)\n",
    "            cost, y_train_collected = losses.nffn_mse_min_dist(y_pred, x_test, y_train, max_y_length, device)\n",
    "            test_loss += cost.item()\n",
    "    avg_test_loss = test_loss / len(test_dataloader_2d)\n",
    "    ffn_costval_test.append(avg_train_loss)\n",
    "\n",
    "    if j % 10 == 0: # Get the loss every 10 epochs\n",
    "        print(f\"Epoch {j}: Train Loss - {avg_train_loss}, Test Loss - {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sanity test for wheter the outputs look appropriate\n",
    "schedule.sanity(tra_ed_model, test_dataloader_3d, device, max_y_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Plot results\n",
    "Here all results from above are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager as fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'serif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feedforward Network, Non-Deep, Medium Dataset, Cross Entropy Loss\n",
    "plt.figure(figsize=(8, 8))\n",
    "x_data = list(range(100))\n",
    "prop = fm.FontProperties(fname='/usr/share/fonts/opentype/freefont/FreeSerif.otf')\n",
    "plt.plot(x_data, FFN_CELtrain, label='Training cross entropy loss')\n",
    "plt.plot(x_data, FFN_CELtest, label='Test cross entropy loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross entropy loss')\n",
    "plt.legend()\n",
    "plt.title(\"Feedforward Network, Non-Deep, Medium Dataset, Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward Network, Non-Deep, Medium Dataset, Accuracy\n",
    "plt.figure(figsize=(8, 8))\n",
    "x_data = list(range(100))\n",
    "plt.plot(x_data, FFN_ACCtrain, label='Training accuracy')\n",
    "plt.plot(x_data, FFN_ACCtest, label='Test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Feedforward Network, Non-Deep, Medium Dataset, Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network, Medium, Medium Dataset, Accuracy, Cross Entropy Loss\n",
    "plt.figure(figsize=(8, 8))\n",
    "x_data = list(range(100))\n",
    "plt.plot(x_data, RNN_CELtrain, label='Training cross entropy loss')\n",
    "plt.plot(x_data, RNN_CELtest, label='Test cross entropy loss')\n",
    "plt.plot(x_data, RNN_ACCtrain, label='Training accuracy')\n",
    "plt.plot(x_data, RNN_ACCtest, label='Test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Transformers results')\n",
    "plt.legend()\n",
    "plt.title(\"Recurrent Neural Network, Medium, Medium Dataset, Accuracy, Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Short-Term Memory, Medium, Medium Dataset, Accuracy, Cross Entropy Loss\n",
    "plt.figure(figsize=(8, 8))\n",
    "x_data = list(range(100))\n",
    "plt.plot(x_data, LSTM_CELtrain, label='Training cross entropy loss')\n",
    "plt.plot(x_data, LSTM_CELtest, label='Test cross entropy loss')\n",
    "plt.plot(x_data, LSTM_ACCtrain, label='Training accuracy')\n",
    "plt.plot(x_data, LSTM_ACCtest, label='Test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Transformers results')\n",
    "plt.legend()\n",
    "plt.title(\"Long Short-Term Memory, Medium, Medium Dataset, Accuracy, Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer, Medium, Medium Dataset, Accuracy, Cross Entropy Loss\n",
    "plt.figure(figsize=(8, 8))\n",
    "x_data = list(range(100))\n",
    "plt.plot(x_data, TRA_CELtrain, label='Training cross entropy loss')\n",
    "plt.plot(x_data, TRA_CELtest, label='Test cross entropy loss')\n",
    "plt.plot(x_data, TRA_ACCtrain, label='Training accuracy')\n",
    "plt.plot(x_data, TRA_ACCtest, label='Test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Transformers results')\n",
    "plt.legend()\n",
    "plt.title(\"Transformer, Medium, Medium Dataset, Accuracy, Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-Accuracy all Networks, Medium Dataset, Accuracy\n",
    "plt.figure(figsize=(8, 8))\n",
    "x_data = list(range(100))\n",
    "plt.plot(x_data, FFN_ACCtest, label='FFN Test accuracy')\n",
    "plt.plot(x_data, RNN_ACCtest, label='RNN Test accuracy')\n",
    "plt.plot(x_data, LSTM_ACCtest, label='LSTM Test accuracy')\n",
    "plt.plot(x_data, TRA_ACCtest, label='Transformer Test accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Transformers results')\n",
    "plt.legend()\n",
    "plt.title(\"Test-Accuracy all Networks, Medium Dataset, Accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
