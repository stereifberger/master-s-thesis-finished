{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Overview\n",
    "An enviroment to train and evaluate neural networks on learning logical consequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Create dataset\n",
    "First the dataset for training is generated. For this the function \"create_dataset\" from \"generation.py\" utilizes the functions \"gen_outp_PA\" to generate a set of random starting formulas, for which iterativly the applicability of rules is checked. All applicable rules are then used to generate new derivations. In each iteration of gen_oupt_PA, set by the iterations variable, new, longer examples are generated.\n",
    "\n",
    "**Rules.** The rules are defined in calculi.py. Two sets are avaiable: Intuitionistic propositional logic (set below via \"calculus = ipl\") and classical propositional logic (set below via \"calculus = cpl\").\n",
    "\n",
    "**Dataset entries.**\n",
    "- **x_train.** Training input: [INDEX, PREMISES, DERIVATION SYMBOL, CONCLUSION]\n",
    "- **y_tdict.** Dictionary of correct derivations for input INDEX: {INDEX: [DERIVATIONS_0...DERIVATION_N]}\n",
    "\n",
    "**Encoding.** Propositional variables and logical constants are encoded as integers. The integers are then one-hot-encoded into unique sequences containing only 0s and ones with the length of the maximum integer value, the feature length. The shape of the individual entries is 2D: [SEQUENCE LENGTH, FEATURE LENGTH].\n",
    "\n",
    "**Example entries withouth numerical representation and one-hot-encoding.**\n",
    "- **x_train.** [2345, A, A THEN B, DERIVES, B OR C]\n",
    "- **y_tdict.** {2345: [[A, A THEN B, B, B OR C], [A, A THEN B, B, A AND B, B OR C]]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "x_train_2d, x_train_3d, y_tdict, max_y_train_len = generation.create_dataset(iterations = [1,2], calculus = calculi.ipl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Prepare dataset and define model for training\n",
    "Next with pytorch's dataloader the single training entries in x_train are assigned to batches of size \"batch size\" in mixed order. Then the different models are defined using definitions from \"architectures.py\". These models are:\n",
    "\n",
    "- Feedforward network (net)\n",
    "- Recurrent neural network (RNNNet)\n",
    "- Long-short-term memory (LSTMNet)\n",
    "- Transformers (TransformerModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the datasets' shapes for th model definitions later\n",
    "two_d_shape = x_train_2d.shape\n",
    "three_d_shape = x_train_3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train-test split to 80-20 and get the 2d dataset's shapes # [^1]\n",
    "train_size = int(0.8 * len(x_train_2d)) \n",
    "test_size = len(x_train_2d) - train_size \n",
    "x_train_2d, x_test_2d = random_split(x_train_2d, [train_size, test_size])\n",
    "train_size = int(0.8 * len(x_train_3d))\n",
    "test_size = len(x_train_3d) - train_size\n",
    "x_train_3d, x_test_3d = random_split(x_train_3d, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect and mix the data in [^2]\n",
    "train_dataloader_2d = DataLoader(dataset = x_train_2d, shuffle = True, batch_size = 50)\n",
    "test_dataloader_2d = DataLoader(dataset = x_test_2d, shuffle = True, batch_size = 50)\n",
    "train_dataloader_3d = DataLoader(dataset = x_train_3d, shuffle = True, batch_size = 50)\n",
    "test_dataloader_3d = DataLoader(dataset = x_test_3d, shuffle = True, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the four models [^3]\n",
    "ffn_model = architectures.ffn(input_size = two_d_shape[1]-1, \n",
    "                              output_size = max_y_train_len)\n",
    "rnn_model = architectures.rnn(input_size = three_d_shape[2],\n",
    "                              hidden_size = 20,\n",
    "                              output_size = max_y_train_len)\n",
    "lst_model = architectures.lst(input_size = three_d_shape[2],\n",
    "                              hidden_size = 20,\n",
    "                              output_size = max_y_train_len)\n",
    "tra_model = architectures.tra(input_size = three_d_shape[2],\n",
    "                              hidden_size = 20,\n",
    "                              output_size = max_y_train_len,\n",
    "                              num_layers = 2,\n",
    "                              nhead = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer to be SGD, with a learning rate of 0.1 [^4]\n",
    "ffn_optimizer = torch.optim.SGD(ffn_model.parameters(),lr=0.001)\n",
    "rnn_optimizer = torch.optim.SGD(rnn_model.parameters(),lr=0.001)\n",
    "lst_optimizer = torch.optim.SGD(lst_model.parameters(),lr=0.001)\n",
    "tra_optimizer = torch.optim.SGD(tra_model.parameters(),lr=0.001)\n",
    "\n",
    "# Train for 1000 epochs\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Training\n",
    "Each subsequent cell trains one of the four models and calculates their mean squared error loss for the nearest correct derivation from the dataset to the derivation provided by the model. The logic for this is impolemented in the custom loss function \"mse_min_dist\" in losses.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6314168195bb4a0b93cdf2f885752bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss - 0.1477228819296278, Test Loss - 0.14763237535953522\n",
      "Epoch 10: Train Loss - 0.14660158034028678, Test Loss - 0.14649446122348309\n",
      "Epoch 20: Train Loss - 0.14555341621925091, Test Loss - 0.1454742718487978\n",
      "Epoch 30: Train Loss - 0.14453560165290175, Test Loss - 0.14443392492830753\n",
      "Epoch 40: Train Loss - 0.1435283643418345, Test Loss - 0.1434400100260973\n",
      "Epoch 50: Train Loss - 0.142530527608148, Test Loss - 0.142438855022192\n",
      "Epoch 60: Train Loss - 0.1415448312101693, Test Loss - 0.14146025478839874\n",
      "Epoch 70: Train Loss - 0.14056763669540143, Test Loss - 0.14048637263476849\n",
      "Epoch 80: Train Loss - 0.13959745292005868, Test Loss - 0.13951341435313225\n",
      "Epoch 90: Train Loss - 0.13863798770411262, Test Loss - 0.1385553739964962\n",
      "Epoch 100: Train Loss - 0.13767967398824363, Test Loss - 0.137598916888237\n",
      "Epoch 110: Train Loss - 0.13673111642229147, Test Loss - 0.13665457628667355\n",
      "Epoch 120: Train Loss - 0.13578921402322836, Test Loss - 0.13571294955909252\n",
      "Epoch 130: Train Loss - 0.13485414714648805, Test Loss - 0.13477670028805733\n",
      "Epoch 140: Train Loss - 0.13392766950459317, Test Loss - 0.1338510662317276\n",
      "Epoch 150: Train Loss - 0.13300324565377727, Test Loss - 0.1329274196177721\n",
      "Epoch 160: Train Loss - 0.13208727281669091, Test Loss - 0.13201113604009151\n",
      "Epoch 170: Train Loss - 0.13117802245863552, Test Loss - 0.1311094257980585\n",
      "Epoch 180: Train Loss - 0.13027464161659108, Test Loss - 0.1302035227417946\n",
      "Epoch 190: Train Loss - 0.1293781081150318, Test Loss - 0.12930882535874844\n"
     ]
    }
   ],
   "source": [
    "### FFN ### [^5]\n",
    "ffn_costval_train = [] # Define the lists for the loss values\n",
    "ffn_costval_test = []\n",
    "for j in tqdm(range(epochs), desc = \"Epoch\"): # Loop over all epochs\n",
    "    ffn_model.train() # Set to training mode (weights are adjusted)\n",
    "    train_loss = 0\n",
    "    for i, x_train in enumerate(train_dataloader_2d):   # Loop over all batches\n",
    "        y_pred = ffn_model(x_train[:,1:])               # Get the model's output for batch \n",
    "        cost = losses.mse_min_dist(y_pred, x_train, y_tdict, (max_y_train_len/14), \"ffn\") # Calculate loss\n",
    "        # Backpropagation\n",
    "        ffn_optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        ffn_optimizer.step()\n",
    "        train_loss += cost.item() # Append loss to intermediary list for average loss calculation\n",
    "    avg_train_loss = train_loss / len(train_dataloader_2d) # Calculate average loss\n",
    "    ffn_costval_train.append(avg_train_loss)\n",
    "\n",
    "    ffn_model.eval() # Set evaluation mode (weights are not adjusted)\n",
    "    test_loss = 0 \n",
    "    # Analog to above but without training a loop over all batches\n",
    "    with torch.no_grad(): \n",
    "        for i, x_test in enumerate(test_dataloader_2d):\n",
    "            y_pred = ffn_model(x_test[:, 1:])\n",
    "            cost = losses.mse_min_dist(y_pred, x_test, y_tdict, (max_y_train_len/14), \"ffn\")\n",
    "            test_loss += cost.item()\n",
    "    avg_test_loss = test_loss / len(test_dataloader_2d)\n",
    "\n",
    "    if j % 10 == 0: # Get the loss every 10 epochs\n",
    "        print(f\"Epoch {j}: Train Loss - {avg_train_loss}, Test Loss - {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RNN ###\n",
    "rnn_costval_train = [] # Define the lists for the loss values\n",
    "rnn_costval_test = []\n",
    "for j in tqdm(range(epochs), desc = \"Epoch\"): # Loop over all epochs\n",
    "    rnn_model.train() # Set to training mode (weights are adjusted)\n",
    "    train_loss = 0\n",
    "    for i, x_train in enumerate(train_dataloader_3d):   # Loop over all batches\n",
    "        y_pred = rnn_model(x_train[:,1:])               # Get the model's output for batch \n",
    "        cost = losses.mse_min_dist(y_pred, x_train, y_tdict, (max_y_train_len/14), \"rnn\") # Calculate loss\n",
    "        # Backpropagation\n",
    "        rnn_optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        rnn_optimizer.step()\n",
    "        train_loss += cost.item() # Append loss to intermediary list for average loss calculation\n",
    "    avg_train_loss = train_loss / len(train_dataloader_3d) # Calculate average loss\n",
    "    rnn_costval_train.append(avg_train_loss)\n",
    "\n",
    "    rnn_model.eval() # Set evaluation mode (weights are not adjusted)\n",
    "    test_loss = 0 \n",
    "    # Analog to above but without training a loop over all batches\n",
    "    with torch.no_grad(): \n",
    "        for i, x_test in enumerate(test_dataloader_3d):\n",
    "            y_pred = rnn_model(x_test[:, 1:])\n",
    "            cost = losses.mse_min_dist(y_pred, x_test, y_tdict, (max_y_train_len/14), \"rnn\")\n",
    "            test_loss += cost.item()\n",
    "    avg_test_loss = test_loss / len(test_dataloader_2d)\n",
    "\n",
    "    if j % 10 == 0: # Get the loss every 10 epochs\n",
    "        print(f\"Epoch {j}: Train Loss - {avg_train_loss}, Test Loss - {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LSTM ###\n",
    "lst_costval_train = [] # Define the lists for the loss values\n",
    "lst_costval_test = []\n",
    "for j in tqdm(range(epochs), desc = \"Epoch\"): # Loop over all epochs\n",
    "    lst_model.train() # Set to training mode (weights are adjusted)\n",
    "    train_loss = 0\n",
    "    for i, x_train in enumerate(train_dataloader_3d):   # Loop over all batches\n",
    "        y_pred = lst_model(x_train[:,1:])               # Get the model's output for batch \n",
    "        cost = losses.mse_min_dist(y_pred, x_train, y_tdict, (max_y_train_len/14), \"lst\") # Calculate loss\n",
    "        # Backpropagation\n",
    "        lst_optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        lst_optimizer.step()\n",
    "        train_loss += cost.item() # Append loss to intermediary list for average loss calculation\n",
    "    avg_train_loss = train_loss / len(train_dataloader_3d) # Calculate average loss\n",
    "    lst_costval_train.append(avg_train_loss)\n",
    "\n",
    "    lst_model.eval() # Set evaluation mode (weights are not adjusted)\n",
    "    test_loss = 0 \n",
    "    # Analog to above but without training a loop over all batches\n",
    "    with torch.no_grad(): \n",
    "        for i, x_test in enumerate(test_dataloader_3d):\n",
    "            y_pred = lst_model(x_test[:, 1:])\n",
    "            cost = losses.mse_min_dist(y_pred, x_test, y_tdict, (max_y_train_len/14), \"lst\")\n",
    "            test_loss += cost.item()\n",
    "    avg_test_loss = test_loss / len(test_dataloader_2d)\n",
    "\n",
    "    if j % 10 == 0: # Get the loss every 10 epochs\n",
    "        print(f\"Epoch {j}: Train Loss - {avg_train_loss}, Test Loss - {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformers ###\n",
    "tra_costval_train = [] # Define the lists for the loss values\n",
    "tra_costval_test = []\n",
    "for j in tqdm(range(epochs), desc = \"Epoch\"): # Loop over all epochs\n",
    "    tra_model.train() # Set to training mode (weights are adjusted)\n",
    "    train_loss = 0\n",
    "    for i, x_train in enumerate(train_dataloader_3d):   # Loop over all batches\n",
    "        y_pred = tra_model(x_train[:,1:])               # Get the model's output for batch \n",
    "        cost = losses.mse_min_dist(y_pred, x_train, y_tdict, (max_y_train_len/14), \"tra\") # Calculate loss\n",
    "        # Backpropagation\n",
    "        tra_optimizer.zero_grad()\n",
    "        cost.backward()\n",
    "        tra_optimizer.step()\n",
    "        train_loss += cost.item() # Append loss to intermediary list for average loss calculation\n",
    "    avg_train_loss = train_loss / len(train_dataloader_3d) # Calculate average loss\n",
    "    tra_costval_train.append(avg_train_loss)\n",
    "\n",
    "    tra_model.eval() # Set evaluation mode (weights are not adjusted)\n",
    "    test_loss = 0 \n",
    "    # Analog to above but without training a loop over all batches\n",
    "    with torch.no_grad(): \n",
    "        for i, x_test in enumerate(test_dataloader_3d):\n",
    "            y_pred = tra_model(x_test[:, 1:])\n",
    "            cost = losses.mse_min_dist(y_pred, x_test, y_tdict, (max_y_train_len/14), \"tra\")\n",
    "            test_loss += cost.item()\n",
    "    avg_test_loss = test_loss / len(test_dataloader_2d)\n",
    "\n",
    "    if j % 10 == 0: # Get the loss every 10 epochs\n",
    "        print(f\"Epoch {j}: Train Loss - {avg_train_loss}, Test Loss - {avg_test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Plot results\n",
    "Here all results from above are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm_costval_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m y_data_ffn \u001b[38;5;241m=\u001b[39m ffn_costval_train\n\u001b[1;32m      4\u001b[0m y_data_rnn \u001b[38;5;241m=\u001b[39m rnn_costval_train\n\u001b[0;32m----> 5\u001b[0m y_data_lst \u001b[38;5;241m=\u001b[39m \u001b[43mlstm_costval_train\u001b[49m\n\u001b[1;32m      6\u001b[0m y_data_tra \u001b[38;5;241m=\u001b[39m tra_costval_train\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(x_data, y_data_ffn, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFFN\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lstm_costval_train' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "x_data = list(range(200))\n",
    "y_data_ffn = ffn_costval_train\n",
    "y_data_rnn = rnn_costval_train\n",
    "y_data_lst = lstm_costval_train\n",
    "y_data_tra = tra_costval_train\n",
    "plt.plot(x_data, y_data_ffn, label='FFN')\n",
    "plt.plot(x_data, y_data_rnn, label='RNN')\n",
    "plt.plot(x_data, y_data_lst, label='LSTM')\n",
    "plt.plot(x_data, y_data_tra, label='Transformers')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Virtual_Kernel",
   "language": "python",
   "name": "virtual_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
