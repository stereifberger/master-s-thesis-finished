from imports import * # Import libraries

# Loss for feed forward networks
def mse_loss_ffn(y_pred, input, outp_dict, max_y_train_len, device):
    batch_losses = []
    model_output_list = []
    ground_truth_list = []
    y_train_collected = torch.empty((0, len(outp_dict[0][0]))).to(device) # Load the ground truth data to the GPU
    for idx, (y_pred_single, input_single) in enumerate(zip(y_pred, input)): # Loop over inputs and outputs of model
        y_train_set = outp_dict[int(input_single[0])] # Get the corresponding correct derivations from ground truth data which is indexed by the first number in the input data

        """
        Description: Calculation of Euclidean Distance between pred tensor and tensor of correct outputs
        Generated by: GPT-4
        URL of Service: https://platform.openai.com/playground
        """
        if y_train_set.nelement() == 0: # If y_train_set is empty, continue to next
            continue
        non_zero_mask = torch.any(y_train_set != 0, dim=1) # Define filter that filters out all empty derivarions
        y_train_set = y_train_set[non_zero_mask] # Apply the filter
        y_pred_single_expanded = y_pred_single.unsqueeze(0).expand_as(y_train_set) # Make the predictions the same dimensionality as the ground truth data
        distances = torch.norm(y_train_set - y_pred_single_expanded, dim=1)  # Calculate Euclidean distance for single output and all correct derivations for given input
        
        min_distanCrossEntropyLossce, min_idx = torch.min(distances, dim=0)  # Find minimal distance
        selected_y_train = y_train_set[min_idx]
        criterion = nn.CrossEntropyLoss()
        loss = criterion(y_pred_single, selected_y_train.float())
        y_train_collected = torch.cat((y_train_collected, selected_y_train.unsqueeze(0)), dim=0)
        batch_losses.append(loss)

    if batch_losses:
        total_loss = torch.stack(batch_losses).mean()  # Stack losses and compute mean
    else:
        total_loss = torch.tensor(0.0, device=y_pred.device)  # No valid losses, return 0
    
    return total_loss, y_train_collected # Return total loss and selected ground truth data

def reverse(inpt): # Reverse tensor to formulas
    inpt = np.argmax(inpt, axis=1) 
    inpt = [calculi.symb_reverse[num.item()] for num in inpt]
    inpt = ''.join(inpt)
    return inpt

# The same as above but for LSTM and Transformer
def mse_loss(y_pred, input, outp_dict, max_y_train_len, device):
    batch_losses = []
    y_train_collected = torch.empty((0, len(outp_dict[0][0]), 14)).to(device)
    for idx, (y_pred_single, input_single) in enumerate(zip(y_pred, input)):
        if input_single.dim() == 2:
            y_train_set = outp_dict[int(input_single[0][0])]
        else:
            y_train_set = outp_dict[int(input_single[0])]
        if y_train_set.nelement() == 0:
            continue
        diff = y_train_set.size(1) - y_pred_single.size(0)
        non_zero_mask = y_train_set.view(outp_dict.shape[1],-1).any(dim=1)
        y_train_set = y_train_set[non_zero_mask]
        y_pred_single = torch.nn.functional.pad(y_pred_single, (0, 0, 0, diff))
        
        target_tensor = y_pred_single.unsqueeze(0)
        distances = torch.norm(y_train_set - y_pred_single, dim=[1, 2])
        min_index = distances.argmin().item()

        selected_y_train = y_train_set[min_index]
        criterion = nn.CrossEntropyLoss()
        
        loss = criterion(y_pred_single, selected_y_train.float())
        y_train_collected = torch.cat((y_train_collected, selected_y_train.unsqueeze(0)), dim=0)
        batch_losses.append(loss)
    if batch_losses:
        total_loss = torch.stack(batch_losses).mean() 
    else:
        total_loss = torch.tensor(0.0, device=y_pred.device)
    
    return total_loss, y_train_collected